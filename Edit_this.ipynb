{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Malik-Zubair123/Gemma2_Hackathon/blob/main/Edit_this.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uQBt1sT87JSf",
    "outputId": "c6257d97-320b-4487-f323-b00781daa5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    }
   ],
   "source": [
    "import pip as pip\n",
    "\n",
    "\n",
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vcuamq6Z7Xw4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrOF2iWY-A-f"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import traceback\n",
    "\n",
    "\n",
    "class TranscriptSummarizer:\n",
    "    def __init__(self, model_name=\"google/gemma-2b\", token=\"Your_Token_Here\"):\n",
    "        \"\"\"\n",
    "        Initialize the summarization model and tokenizer with Hugging Face authentication.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Hugging Face model to use for summarization.\n",
    "            token (str): Hugging Face API token.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set the Hugging Face token for authentication\n",
    "            from huggingface_hub import login\n",
    "            login(token)  # Authenticate using the token\n",
    "\n",
    "            # Load tokenizer and model with specific configurations\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "            # Set pad token if not already set\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Model loading error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def preprocess_text(self, text, max_length=2048):\n",
    "        \"\"\"\n",
    "        Preprocess input text for summarization with improved truncation.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to preprocess.\n",
    "            max_length (int): Maximum token length to allow.\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned and processed text.\n",
    "        \"\"\"\n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = ' '.join(text.split())\n",
    "\n",
    "        # Truncate text to manageable length\n",
    "        tokens = self.tokenizer.encode(\n",
    "            cleaned_text,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return self.tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    def generate_summary(self, text, max_new_tokens=150):\n",
    "        \"\"\"\n",
    "        Generate summary using the loaded model with enhanced error handling.\n",
    "\n",
    "        Args:\n",
    "            text (str): Text to summarize.\n",
    "            max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            str: Generated summary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Preprocess the text with length control\n",
    "            processed_text = self.preprocess_text(text)\n",
    "\n",
    "            print(f\"Processed Text Size: {len(processed_text)} characters\")\n",
    "\n",
    "            # Construct a prompt that explicitly requests a summary\n",
    "            summary_prompt = f\"Summarize the following text concisely:\\n{processed_text}\\n\\nSummary:\"\n",
    "\n",
    "            # Prepare inputs with error checking\n",
    "            inputs = self.tokenizer(\n",
    "                summary_prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=2048,\n",
    "                padding=True\n",
    "            )\n",
    "\n",
    "            # Generate summary with more robust parameters\n",
    "            summary_ids = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_return_sequences=1,\n",
    "                no_repeat_ngram_size=2,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            # Decode the summary\n",
    "            summary = self.tokenizer.decode(\n",
    "                summary_ids[0][inputs['input_ids'].shape[1]:],  # Only decode new generated tokens\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            return summary.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Detailed Summary Generation Error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return f\"Could not generate summary. Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# Function to handle different input scenarios\n",
    "def summarize_text(input_text=None, file_path=None, max_new_tokens=150):\n",
    "    \"\"\"\n",
    "    Flexible function to summarize text from different sources.\n",
    "\n",
    "    Args:\n",
    "        input_text (str, optional): Direct text input.\n",
    "        file_path (str, optional): Path to text file.\n",
    "        max_new_tokens (int, optional): Maximum new tokens in summary.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated summary.\n",
    "    \"\"\"\n",
    "    # Initialize summarizer\n",
    "    summarizer = TranscriptSummarizer()\n",
    "\n",
    "    # Determine text source\n",
    "    if input_text:\n",
    "        text = input_text\n",
    "    elif file_path:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file: {e}\")\n",
    "            return f\"File reading error: {e}\"\n",
    "    else:\n",
    "        return \"No text provided. Please give input text or file path.\"\n",
    "\n",
    "    # Generate and return summary\n",
    "    return summarizer.generate_summary(text, max_new_tokens)\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Option 1: Directly input text\n",
    "    sample_text = \"\"\"\n",
    "    Key Advantages of Plain Text Files:\n",
    "    Plain text will never require a subscription, lock away features, or go out of business. It's free and here forever.\n",
    "\n",
    "    Flexibility: Open Them Anywhere: Plain text files are the most flexible file format we have. They can be opened by hundreds, if not thousands, of applications. Since they are a basic component of computers, you can even open them on the computer command line.\n",
    "    No New Tools: We all get excited by new stuff, and for productivity junkies, we perk up with excitement to try out a new piece of software. Unfortunately tools come and go, and switching system can be wasteful and counterproductive. Switching to plain text means you never need to migrate to a new tool.\n",
    "    Portability: By portability I mean that your files can be moved to and from different operating systems, platforms and devices and you can still open them. Whether you are on a Mac, Linux, Windows or some future tool, you'll be able to open and edit your plain text files there.\n",
    "    Future-Proof: A while back I discovered several writings I did in high school using Windows 95 and a version of Microsoft Word. While I was able to eventually open the files, the formatting had been lost. This revealed a potential danger in locking your words into a format that may not be around forever. Fortunately, while other file formats may come and go, plain text files will remain.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate summary from text\n",
    "    summary = summarize_text(input_text=sample_text, max_new_tokens=100)\n",
    "    print(\"Generated Summary:\")\n",
    "    print(summary)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UI Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit\n",
    "!pip install pyngrok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Define the file path dynamically\n",
    "file_path = '/content/model.py'\n",
    "\n",
    "# Check if the file exists before loading\n",
    "if os.path.exists(file_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"model\", file_path)\n",
    "    model = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[\"model\"] = model\n",
    "    spec.loader.exec_module(model)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"{file_path} not found\")\n",
    "\n",
    "from model import TranscriptSummarizer\n",
    "import streamlit as st\n",
    "from io import StringIO\n",
    "import PyPDF2\n",
    "\n",
    "# Set up the Streamlit app configuration\n",
    "st.set_page_config(page_title=\"Meeting Summarizer AI\", page_icon=\":📝\", layout=\"wide\", initial_sidebar_state=\"auto\", menu_items=None)\n",
    "\n",
    "# Header and Subheader\n",
    "st.header(\"Meeting Summarizer AI\", anchor=\"header\")\n",
    "st.subheader(\"Upload transcripts and get actionable insights in seconds!\", anchor=\"subheader\")\n",
    "\n",
    "# File upload widget\n",
    "uploadFile = st.file_uploader(\"Upload your meeting transcript\", type=[\"txt\", \"pdf\"], accept_multiple_files=True)\n",
    "\n",
    "# Text input area\n",
    "transcript_input = st.text_area(\"Enter your transcript here:\", placeholder=\"Meeting minutes...\", height=200)\n",
    "\n",
    "# Initialize variable for the transcript content\n",
    "input_text = None\n",
    "\n",
    "summarizer = TranscriptSummarizer()\n",
    "\n",
    "# Function to handle PDF text extraction\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    try:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error extracting text from PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process file upload or use text input\n",
    "if uploadFile is not None:\n",
    "    input_text = \"\"\n",
    "    for file in uploadFile:\n",
    "        if file.type == \"text/plain\":\n",
    "            input_text += file.read().decode(\"utf-8\")\n",
    "        elif file.type == \"application/pdf\":\n",
    "            # Extract text from PDF\n",
    "            pdf_text = extract_text_from_pdf(file)\n",
    "            if pdf_text:\n",
    "                input_text += pdf_text\n",
    "else:\n",
    "    input_text = transcript_input\n",
    "\n",
    "# Button to trigger summarization\n",
    "summarizeButton = st.button(\"Summarize\")\n",
    "\n",
    "# If the button is clicked, generate and display summary\n",
    "if summarizeButton:\n",
    "    if input_text:\n",
    "        try:\n",
    "            # Step 1: Preprocess the text\n",
    "            preprocessed_text = summarizer.preprocess_text(input_text)\n",
    "\n",
    "            # Display preprocessed text (for debugging or validation)\n",
    "            st.subheader(\"Preprocessed Text:\")\n",
    "            st.write(preprocessed_text)\n",
    "\n",
    "            # Step 2: Generate the summary\n",
    "            summary = summarizer.generate_summary(input_text)\n",
    "\n",
    "            # Display the generated summary\n",
    "            st.subheader(\"Generated Summary:\")\n",
    "            st.write(summary)\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error during summarization: {str(e)}\")\n",
    "    else:\n",
    "        st.warning(\"Please provide a transcript either by uploading a file or entering text.\")\n",
    "\n",
    "# Download button for the generated summary\n",
    "if input_text and summarizeButton:\n",
    "    st.download_button(\n",
    "        label=\"Download Summary\",\n",
    "        data=summary,\n",
    "        file_name=\"summary.txt\",\n",
    "        mime=\"text/plain\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "import os\n",
    "\n",
    "# Set ngrok authentication token (replace with your token)\n",
    "ngrok.set_auth_token(\"Your_ngrok_token\")\n",
    "\n",
    "# Open the tunnel to Streamlit port 8501\n",
    "public_url = ngrok.connect(8501)\n",
    "\n",
    "# Start Streamlit app in the background\n",
    "os.system(\"streamlit run model.py &\")\n",
    "\n",
    "# Print the public URL for accessing the app\n",
    "print(f\"Streamlit app is running at: {public_url}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPtrmcldPhzQbc5OI6oWb7+",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bf2267d33f84efcb7f4cb360d87c037": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a726210c9252488db201c0777fcfd553",
      "placeholder": "​",
      "style": "IPY_MODEL_60205bd77f9642088a69ed5b91b864ba",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "546c3f71ce8a40a396d8b883362bff61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c1057637daf41b9a45524272d354bf9",
      "placeholder": "​",
      "style": "IPY_MODEL_7594b667d91040ffa0ac21ef1019a78d",
      "value": " 2/2 [00:25&lt;00:00, 10.80s/it]"
     }
    },
    "5d56b9b30d02409aa0c77941abc71152": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2bf2267d33f84efcb7f4cb360d87c037",
       "IPY_MODEL_6543946fb1984137a022a68ff5aba430",
       "IPY_MODEL_546c3f71ce8a40a396d8b883362bff61"
      ],
      "layout": "IPY_MODEL_6a9a4804e3aa40afb649d0d36cba6be1"
     }
    },
    "60205bd77f9642088a69ed5b91b864ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6543946fb1984137a022a68ff5aba430": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f24639f0715f4694a61499c5abb8dc9d",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8d9fa25f33984abe93f8c4ecb0457af1",
      "value": 2
     }
    },
    "6a9a4804e3aa40afb649d0d36cba6be1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c1057637daf41b9a45524272d354bf9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7594b667d91040ffa0ac21ef1019a78d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d9fa25f33984abe93f8c4ecb0457af1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a726210c9252488db201c0777fcfd553": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f24639f0715f4694a61499c5abb8dc9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
